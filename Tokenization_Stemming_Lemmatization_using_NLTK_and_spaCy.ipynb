{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenelaineDC/Natural-Language-Processing/blob/main/Tokenization_Stemming_Lemmatization_using_NLTK_and_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3b2ea0f",
      "metadata": {
        "id": "d3b2ea0f"
      },
      "source": [
        "\n",
        "# NLP Tokenization & Preprocessing\n",
        "\n",
        "This notebook provides a comprehensive introduction to text tokenization and preprocessing in NLP using NLTK and spaCy. It is designed for learners who want to understand both the theory and practical implementation of preparing text for analysis or machine learning.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c4f0bca",
      "metadata": {
        "id": "2c4f0bca"
      },
      "source": [
        "\n",
        "## Tokenization\n",
        "Tokenization is the process of splitting text into smaller parts called **tokens** ‚Äî usually words or sentences.\n",
        "\n",
        "**Example:**\n",
        "```\n",
        "\"I love NLP!\" ‚Üí [\"I\", \"love\", \"NLP\", \"!\"]\n",
        "```\n",
        "\n",
        "**Why It's Important:**\n",
        "- Computers can't directly understand text; they need structured input.  \n",
        "- Tokenization helps us break down text for analysis (e.g., counting words, building vocabularies).  \n",
        "- It‚Äôs the *first step* in almost every NLP pipeline.\n",
        "\n",
        "**Two submodules:**\n",
        "- Sentence Tokenizer: A sentence tokenizer splits a piece of text into sentences. Each resulting token is a complete sentence, not individual words.\n",
        "- Work Tokenizer: A word tokenizer splits a sentence (or text) into individual words or tokens. Punctuation often becomes a separate token.\n",
        "\n",
        "| Feature | Sentence Tokenizer                                              | Word Tokenizer                                                                |\n",
        "| ------- | --------------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| Output  | Sentences                                                       | Words / tokens                                                                |\n",
        "| Level   | Coarse                                                          | Fine-grained                                                                  |\n",
        "| Purpose | Document-level analysis, sentence classification, summarization | Word-level analysis, feature extraction, embeddings                           |\n",
        "| Example | `\"Hello world! I am NLP.\"` ‚Üí `[\"Hello world!\", \"I am NLP.\"]`    | `\"Hello world! I am NLP.\"` ‚Üí `['Hello', 'world', '!', 'I', 'am', 'NLP', '.']` |\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25276466",
      "metadata": {
        "id": "25276466"
      },
      "source": [
        "\n",
        "### Using NLTK\n",
        "- `sent_tokenize()` ‚Üí splits text into sentences using punctuation and capitalization cues.\n",
        "- `word_tokenize()` ‚Üí splits sentences into words, handling punctuation and contractions.\n",
        "- `nltk.download('punkt')` ‚Üí downloads models that help NLTK identify sentence boundaries.\n",
        "\n",
        "\n",
        "NLTK‚Äôs tokenization functions (like `word_tokenize()` and `sent_tokenize()`) rely on a pre-trained **Punkt model** to correctly identify:\n",
        "- where sentences start and end, and\n",
        "- how to split text into individual words.\n",
        "\n",
        "So before you can use those tokenizers, you need to make sure the Punkt model is available ‚Äî hence the download.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X1ppAiNRSYQk",
        "outputId": "bb342aef-8fdd-4d67-fbeb-f7180fd2684e"
      },
      "id": "X1ppAiNRSYQk",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download required tokenizer data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyaiZ8fNSh7i",
        "outputId": "4ea62873-a27a-4af6-8422-d767c6480111"
      },
      "id": "OyaiZ8fNSh7i",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "28481eb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28481eb0",
        "outputId": "9352cd68-b53e-4dd0-fe5e-61f11a44f292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['Hello world!', \"I'm learning Natural Language Processing using NLTK.\"]\n",
            "\n",
            "Word Tokenization:\n",
            "['Hello', 'world', '!', 'I', \"'m\", 'learning', 'Natural', 'Language', 'Processing', 'using', 'NLTK', '.']\n"
          ]
        }
      ],
      "source": [
        "# Sample text\n",
        "text = \"Hello world! I'm learning Natural Language Processing using NLTK.\"\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenize(text)\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Spacy\n",
        "\n",
        "- `spacy.load(\"en_core_web_sm\")` loads the English language model.  \n",
        "- `nlp(text)` runs the text through spaCy‚Äôs pipeline, returning a `Doc` object.  \n",
        "- `doc.sents` gives sentences.  \n",
        "- `[token.text for token in doc]` extracts each token (word, punctuation, etc.)."
      ],
      "metadata": {
        "id": "CDR3S1PnVRwJ"
      },
      "id": "CDR3S1PnVRwJ"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gENNxtXOVz9K",
        "outputId": "b8371e56-c6da-4ae0-95f4-2bc502263d32"
      },
      "id": "gENNxtXOVz9K",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "hRzdYueYV5ey"
      },
      "id": "hRzdYueYV5ey",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d8369a49",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8369a49",
        "outputId": "8fa40222-7104-4bd7-ac64-2f7dd70ddfa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "Hello world!\n",
            "I'm learning Natural Language Processing using spaCy.\n",
            "\n",
            "Word Tokenization:\n",
            "['Hello', 'world', '!', 'I', \"'m\", 'learning', 'Natural', 'Language', 'Processing', 'using', 'spaCy', '.']\n"
          ]
        }
      ],
      "source": [
        "# Load small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello world! I'm learning Natural Language Processing using spaCy.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Sentence Tokenization\n",
        "print(\"Sentence Tokenization:\")\n",
        "for sent in doc.sents:\n",
        "    print(sent.text)\n",
        "\n",
        "# Word Tokenization\n",
        "words = [token.text for token in doc]\n",
        "print(\"\\nWord Tokenization:\")\n",
        "print(words)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee68ee37",
      "metadata": {
        "id": "ee68ee37"
      },
      "source": [
        "\n",
        "## NLTK vs spaCy\n",
        "\n",
        "| Feature | **NLTK** | **spaCy** |\n",
        "|----------|-----------|-----------|\n",
        "| **Focus** | Academic / teaching | Production / industrial |\n",
        "| **Speed** | Slower | Faster (Cython backend) |\n",
        "| **Ease of Use** | Simple | More powerful |\n",
        "| **Sentence Parsing** | Basic | Context-aware |\n",
        "| **Integration** | Works with NLTK modules | Works with full NLP pipeline |\n",
        "\n",
        "‚úÖ Use **NLTK** for learning and research.  \n",
        "‚úÖ Use **spaCy** for larger, production-ready NLP applications.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "90879712",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90879712",
        "outputId": "ef5438d1-9aa8-43c6-9724-84c08d72b2d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Tokens:\n",
            " ['Dr.', 'Smith', \"'s\", 'email', 'is', 'dr.smith', '@', 'example.com', '!', 'Let', \"'s\", 'meet', 'at', '10:30', 'a.m.', ',', 'okay', '?']\n",
            "\n",
            "spaCy Tokens:\n",
            " ['Dr.', 'Smith', \"'s\", 'email', 'is', 'dr.smith@example.com', '!', 'Let', \"'s\", 'meet', 'at', '10:30', 'a.m.', ',', 'okay', '?']\n"
          ]
        }
      ],
      "source": [
        "# üß™ Practice Exercise\n",
        "\n",
        "text = \"Dr. Smith's email is dr.smith@example.com! Let's meet at 10:30 a.m., okay?\"\n",
        "\n",
        "# NLTK Tokenization\n",
        "nltk_tokens = word_tokenize(text)\n",
        "\n",
        "# spaCy Tokenization\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "\n",
        "print(\"NLTK Tokens:\\n\", nltk_tokens)\n",
        "print(\"\\nspaCy Tokens:\\n\", spacy_tokens)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing After Tokenization\n",
        "\n",
        "After tokenization, our text is split into words ‚Äî but these words are still messy and inconsistent.\n",
        "For example:\n",
        "```\n",
        "\"Cats\", \"cat\", \"CAT\", \"cats.\"\n",
        "```\n",
        "\n",
        "All these mean the same thing, but they look different to a computer.\n",
        "Text preprocessing helps by cleaning and normalizing the text, so the model can focus on meaning instead of surface differences.\n",
        "\n",
        "Common steps include:\n",
        "\n",
        "| Step                         | Purpose                                                                   |\n",
        "| ---------------------------- | ------------------------------------------------------------------------- |\n",
        "| **Lowercasing**              | Makes the text consistent (so ‚ÄúCat‚Äù and ‚Äúcat‚Äù are treated the same)       |\n",
        "| **Removing punctuation**     | Removes irrelevant characters that don‚Äôt carry meaning                    |\n",
        "| **Removing stopwords**       | Filters out common words (‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù) that add little information |\n",
        "| **Stemming / Lemmatization** | Reduces words to their base form (so ‚Äúrunning‚Äù, ‚Äúruns‚Äù ‚Üí ‚Äúrun‚Äù)           |\n",
        "\n",
        "\n",
        "Quick Guidelines:\n",
        "\n",
        "| Step                   | Needed?     | Comment                                                        |\n",
        "| ---------------------- | ----------- | -------------------------------------------------------------- |\n",
        "| Lowercasing            | Usually yes | Ensures consistency, especially for Bag of Words/TF-IDF        |\n",
        "| Punctuation removal    | Depends     | Safe for classification/search; avoid for syntax/NER/sentiment |\n",
        "| Stopword removal       | Often       | Reduces noise for frequency-based features                     |\n",
        "| Stemming/Lemmatization | Optional    | Helps normalize words for simpler models                       |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vnwr7SGnY1t0"
      },
      "id": "vnwr7SGnY1t0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lowercasing and Punctuation Removal\n",
        "\n",
        "- Lowercasing is important because NLP models often treat `\"Cat\"` and `\"cat\"` as different tokens.\n",
        "- Many models or algorithms (like Bag of Words, TF-IDF, or simple frequency counts) are case-sensitive. So `\"Cat\" ‚â† \"cat\"` unless you normalize the text.\n",
        "- Exceptions:\n",
        "  - Some advanced models (like BERT, spaCy embeddings, transformers) use subword tokenization or case-aware models, which can handle mixed-case text. In those cases, you might not need to lowercase manually.\n",
        "\n",
        "- **‚úÖ Rule of thumb**:\n",
        "  - For traditional NLP pipelines, always lowercase. For modern transformers, check if the model is case-sensitive (bert-base-cased vs bert-base-uncased).\n",
        "\n",
        "### Punctuation Removal\n",
        "\n",
        "- For tasks like word frequency, Bag of Words, or TF-IDF, punctuation doesn‚Äôt carry meaning and may just create noisy tokens (`\"word.\"` ‚â† `\"word\"`).\n",
        "- Exceptions:\n",
        "  - Some NLP tasks rely on punctuation, e.g., sentiment analysis (`\"I love it!\"` vs `\"I love it.\"`) or NLP parsing tasks.\n",
        "  - Libraries like spaCy already separate punctuation into its own token, so `\"Hello!\"` becomes `[\"Hello\", \"!\"]`. This makes it easier to either keep or remove punctuation depending on the task.\n",
        "- **‚úÖ Rule of thumb**:\n",
        "  - If your task is text classification, search, or topic modeling, it‚Äôs safe to remove punctuation.\n",
        "  - If your task is syntax analysis, named entity recognition, or sentiment analysis, consider keeping punctuation."
      ],
      "metadata": {
        "id": "h1nL21drdp8n"
      },
      "id": "h1nL21drdp8n"
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Cats are running faster than the dogs were. They aren't stopping anytime soon!\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Lowercasing\n",
        "words_lower = [w.lower() for w in words]\n",
        "\n",
        "# Remove punctuation\n",
        "words_no_punct = [w for w in words_lower if w not in string.punctuation]\n",
        "\n",
        "print(\"After Lowercasing & Removing Punctuation:\")\n",
        "print(words_no_punct)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEIFZwUhcao9",
        "outputId": "32279a2c-4b3e-492a-e835-23df363c0847"
      },
      "id": "UEIFZwUhcao9",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Lowercasing & Removing Punctuation:\n",
            "['cats', 'are', 'running', 'faster', 'than', 'the', 'dogs', 'were', 'they', 'are', \"n't\", 'stopping', 'anytime', 'soon']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Word Removal\n",
        "\n",
        "Stopwords are very common words in a language that often carry little meaning on their own:\n",
        "Examples:\n",
        "```\n",
        "\"the\", \"is\", \"and\", \"in\", \"to\"\n",
        "```\n",
        "\n",
        "Why remove stopwords?\n",
        "- Reduces noise in Bag of Words or TF-IDF models.\n",
        "- Reduces dimensionality in text features.\n",
        "- Improves performance for tasks like text classification or topic modeling.\n",
        "\n",
        "When NOT to remove stopwords:\n",
        "- When context matters: sentiment analysis or language modeling, since \"not\" changes meaning.\n",
        "- When using transformer models, as they usually learn importance weights internally.\n",
        "\n",
        "‚úÖ Rule of thumb:\n",
        "- Remove stopwords for classical NLP pipelines;\n",
        "- keep them for deep learning or context-sensitive tasks."
      ],
      "metadata": {
        "id": "Iaq78rK6duYo"
      },
      "id": "Iaq78rK6duYo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using NLTK"
      ],
      "metadata": {
        "id": "yEUSGswKiiz7"
      },
      "id": "yEUSGswKiiz7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Display all stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TCDkZjI8nM85",
        "outputId": "b89ecdf0-91e4-4fda-8635-cc1e40c1d9f1"
      },
      "id": "TCDkZjI8nM85",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a simple example to demonstrate stopword removal using NLTK.\"\n",
        "\n",
        "# Tokenize\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Lowercase (optional but recommended)\n",
        "words_lower = [w.lower() for w in words]\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [w for w in words_lower if w not in stop_words]\n",
        "\n",
        "print(\"Original words:\")\n",
        "print(words_lower)\n",
        "print(\"\\nAfter stopword removal (NLTK):\")\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQGd9YhYgM0v",
        "outputId": "127746fe-74eb-42fd-da4b-983953c45e3d"
      },
      "id": "EQGd9YhYgM0v",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words:\n",
            "['this', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'stopword', 'removal', 'using', 'nltk', '.']\n",
            "\n",
            "After stopword removal (NLTK):\n",
            "['simple', 'example', 'demonstrate', 'stopword', 'removal', 'using', 'nltk', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using spaCy\n",
        "\n",
        "Yes! üåø spaCy has built-in stopword removal functionality. You don‚Äôt need to download anything extra like in NLTK ‚Äî spaCy already comes with a comprehensive stopword list for each language model. It has also a built-in lowercase"
      ],
      "metadata": {
        "id": "UVPdaKP6h_0R"
      },
      "id": "UVPdaKP6h_0R"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "Jnsao7dapblg"
      },
      "id": "Jnsao7dapblg",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of default stop words\n",
        "len(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rucOsu5rndwb",
        "outputId": "d4862f71-6ddf-469c-dfc9-f270471688fc"
      },
      "id": "rucOsu5rndwb",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can check a word that is stopword or not by using vocab method\n",
        "print(nlp.vocab['myself'].is_stop)      # check \"myself\" if stopword or not\n",
        "\n",
        "print(nlp.vocab['mystery'].is_stop)     # check \"mystery\" if stopword or not"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnw5E7j3nnl5",
        "outputId": "e6194835-8641-48f6-e354-685f3e3eef12"
      },
      "id": "mnw5E7j3nnl5",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new stopword\n",
        "\n",
        "# Add the word to the set of stop words. Use lowercase!\n",
        "nlp.Defaults.stop_words.add('mystery')\n",
        "# Set the stop_word tag on the lexeme\n",
        "nlp.vocab['mystery'].is_stop = True\n",
        "\n",
        "print(\"New Stopword length: \", len(nlp.Defaults.stop_words))\n",
        "print(\"Mystery a stopword? \", nlp.vocab['mystery'].is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuK3iA9BoG4i",
        "outputId": "21c7262a-d85d-4536-ca60-171e8a3d8be3"
      },
      "id": "GuK3iA9BoG4i",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Stopword length:  327\n",
            "Mystery a stopword?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove a stopword\n",
        "\n",
        "# Remove the word from the set of stop words\n",
        "nlp.Defaults.stop_words.remove('mystery')\n",
        "# Remove the stop_word tag from the lexeme\n",
        "nlp.vocab['mystery'].is_stop = False\n",
        "\n",
        "print(\"New Stopword length: \", len(nlp.Defaults.stop_words))\n",
        "print(\"Mystery a stopword? \", nlp.vocab['mystery'].is_stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4ObCqBBpAcF",
        "outputId": "c94e2397-ffcb-4e07-b1c8-1a6ef5ee5389"
      },
      "id": "T4ObCqBBpAcF",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Stopword length:  326\n",
            "Mystery a stopword?  False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a simple example to demonstrate stopword removal using spaCy.\"\n",
        "\n",
        "# Process text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Remove stopwords and punctuation\n",
        "filtered_words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"After stopword removal (spaCy):\")\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIzT4-3Cg-jU",
        "outputId": "5881d8e4-6752-4329-f5a9-2eda8611ce0f"
      },
      "id": "JIzT4-3Cg-jU",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stopword removal (spaCy):\n",
            "['simple', 'example', 'demonstrate', 'stopword', 'removal', 'spaCy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH_JppL_h5JV",
        "outputId": "e1a3a632-6a73-4fce-b62f-555e6f6e32e8"
      },
      "id": "aH_JppL_h5JV",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming and Lemmatization\n",
        "\n",
        "Both stemming and lemmatization reduce words to their root forms ‚Äî but they do it differently.\n",
        "\n",
        "| Feature        | **Stemming**                                    | **Lemmatization**                         |\n",
        "| -------------- | ----------------------------------------------- | ----------------------------------------- |\n",
        "| **Definition** | Cuts off word endings using simple rules        | Converts word to its dictionary base form |\n",
        "| **Example**    | ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù, ‚Äústudies‚Äù ‚Üí ‚Äústudi‚Äù          | ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù, ‚Äústudies‚Äù ‚Üí ‚Äústudy‚Äù    |\n",
        "| **Accuracy**   | Faster but less accurate (can create non-words) | Slower but linguistically correct         |\n",
        "| **Library**    | `nltk.PorterStemmer`                            | `spacy` or `WordNetLemmatizer`            |\n"
      ],
      "metadata": {
        "id": "XGGoaKMsakC4"
      },
      "id": "XGGoaKMsakC4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using NLTK\n",
        "\n",
        "- NLTK can do both stemming and lemmatization, but lemmatization requires the WordNet resource."
      ],
      "metadata": {
        "id": "TlStzCOhk8KZ"
      },
      "id": "TlStzCOhk8KZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = [\"running\", \"runs\", \"runner\", \"studies\", \"studying\"]\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words (NLTK):\", stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_bpYDGiY-Rw",
        "outputId": "6ea74576-257e-41e8-f18f-9e44f48f158d"
      },
      "id": "c_bpYDGiY-Rw",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'runner', 'studies', 'studying']\n",
            "Stemmed words (NLTK): ['run', 'run', 'runner', 'studi', 'studi']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"runs\", \"runner\", \"studies\", \"studying\"]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # 'v' = verb\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Lemmatized words (NLTK):\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zcT152Ck1iu",
        "outputId": "0c0ca11d-7973-45a8-d151-7f164cfd8c2c"
      },
      "id": "8zcT152Ck1iu",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['running', 'runs', 'runner', 'studies', 'studying']\n",
            "Lemmatized words (NLTK): ['run', 'run', 'runner', 'study', 'study']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Using spaCy\n",
        "\n",
        "- spaCy does not provide stemming, only lemmatization."
      ],
      "metadata": {
        "id": "nG7gNjNUlAYL"
      },
      "id": "nG7gNjNUlAYL"
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"running runs runner studies studying\"\n",
        "doc = nlp(text)\n",
        "\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "print(\"Original text:\", text)\n",
        "print(\"Lemmatized words (spaCy):\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOyFL-mIk7Kd",
        "outputId": "67bf7747-5549-433f-c52e-658ed00ea9ea"
      },
      "id": "ZOyFL-mIk7Kd",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: running runs runner studies studying\n",
            "Lemmatized words (spaCy): ['run', 'run', 'runner', 'study', 'study']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}